{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8271eca4-8a8f-4e71-8299-be4bf5e6f732",
   "metadata": {},
   "source": [
    "# Why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81829f1-82d9-40b9-a5ed-26ed0757b141",
   "metadata": {},
   "source": [
    "Fitting a single decision tree instance may often lead us to low bias and high variance, in other words, overfitting the dataset, even with tree constraints like `min_samples_leaf`, `min_samples_split` or `max_depth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416885e-19a3-4507-b77b-51728922fe50",
   "metadata": {},
   "source": [
    "# Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07300fa7-d942-49c3-873f-891c54118699",
   "metadata": {},
   "source": [
    "<img src=\"random-forests.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228ec18-ce80-48a2-b4fb-787e33692cb0",
   "metadata": {},
   "source": [
    "Fitting many meta learners, in this case, decision trees that only consider a random sample of features, and bagging them to aggregate to the output, will vastly reduces the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c55c35-3568-4076-8f52-38564dc9b874",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36e76e-2992-462b-9e3a-b5c998a74c4e",
   "metadata": {},
   "source": [
    "## Motiviation of average\n",
    "\n",
    "Consider a set of uncorrelated random variables $\\{Y_i\\}_{i=1}^{n}$ with mean $\\mu$ and variance $\\sigma^2$, where $Y_i$ can be treated as prediction from model $i$, the expectation and the variance of the average is\n",
    "$$E[\\frac{1}{n}\\sum_{i=1}^{n}Y_i] = \\frac{1}{n}\\sum_{i=1}^{n}E[Y_i] = \\frac{1}{n} \\cdot n \\cdot \\mu = \\mu$$\n",
    "\n",
    "$$Var[\\frac{1}{n}\\sum_{i=1}^{n}Y_i] = \\frac{1}{n^2}\\sum_{i=1}^{n}Var[Y_i] = \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "Hence, the $\\mu$ stays the same, meaning we won't lose precision in bias, while the variance becomes much smaller\n",
    "\n",
    "In the real world, we won't be able to have uncorrelated models to ensemble. Assume $\\forall i \\neq j, Corr(Y_{i}, Y_{j}) = \\rho$. Then the variance of the average becomes:\n",
    "$$\n",
    "Var[\\frac{1}{n}\\sum_{i=1}^{n}Y_i] = \\frac{1}{n^2}Var[\\sum_{i=1}^{n}Y_i] = \\frac{1}{n^2}(\\sum_{i=1}^{n}Var[Y_i] + \\mathop{\\sum\\sum}_{i \\neq j}Cov(Y_{i}, Y_{j})) = \\frac{\\sigma^2}{n} + \\frac{n(n-1)\\sigma^2\\rho}{n^2} = \\rho\\sigma^2 + \\frac{1-\\rho}{n}\\sigma^2\n",
    "$$\n",
    "As $n$ grows, the first term dominates, so variance still gets effectively reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43553bb4-a67c-4674-9458-50da97e8aed8",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap AGGregatING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca3a2e-8b14-4f38-bfdf-d94845f056d8",
   "metadata": {},
   "source": [
    "Given a training set size $n$, generate $T$ random samples, each size of $n'$ by sampling with replacement.\n",
    "\n",
    "When $n = n'$, $63\\%$ of the data are chosen, while $37\\%$ being OOB (out-of-bag) samples:\n",
    "\n",
    "$$(1 - \\frac{1}{n})^n \\approx \\lim_{n \\to \\infty}(1 - \\frac{1}{n})^n = \\frac{1}{e} = 0.367$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb5c6d-66f2-4cf7-9f0b-4b74379f9e5e",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76805f-3f5f-44c7-b85c-7290a5430b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
