{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bd0746-8b51-4870-ab5c-5fe606272ec0",
   "metadata": {},
   "source": [
    "# Motiviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d254b4-9d60-41a9-be09-1aeec2f433d7",
   "metadata": {},
   "source": [
    "We already have a decision tree $F$ implemented, however it is impossible to learn all samples correctly in one pass, even with infinitely deep depth of the tree. How to improve without any modification to $F$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5feda5-ba8f-47a6-a63c-28452c4e0ebf",
   "metadata": {},
   "source": [
    "The idea of an additive model comes into mind. Can we find any $h$ such that $y = F(x) + h(x)$, where $h$ focuses on the wrong predictions that $F$ makes. If found $h$ is unsatisfactory, we can just rinse and repeat by making new $F = F + h$ and find a new $h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8aaf-af85-423b-b74b-d6099e7f9800",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc6c2c-d355-416f-88b3-347b3b5b8ff5",
   "metadata": {},
   "source": [
    "Say the goal is to minimize a loss function $J$, we can have the derivation as\n",
    "$$J(y, F) = \\sum_i^n L\\big(y_i, F(x_i)\\big)$$\n",
    "where $L$ is a loss function comparing between ground truths and model's prediction.\n",
    "Employing gradient descent is probably a good idea here. Therefore we have,\n",
    "$$F_b(x_i) = F_{b-1}(x_i) - \\eta \\times \\nabla L\\big(y_i, F(x_i)\\big)$$\n",
    "where $F_b$ is our model at step $b$, $\\eta$ is the learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e376c6-fb23-48fe-9bde-f9cd4dee84bd",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "When it is regression, we are considering $L$ to be MSE:\n",
    "$$L\\big(y_i, F(x_i)\\big) = \\frac{1}{2} \\big( y_i - F(x_i) \\big)^2$$\n",
    "Taking the gradient, we would have\n",
    "$$\\frac{ \\partial L\\big(y_i, F(x_i)\\big) }{ \\partial F(x_i) } = \\frac{ \\partial \\frac{1}{2} \\big( y_i - F(x_i) \\big)^2 }{ \\partial F(x_i) } = F(x_i) - y_i$$\n",
    "\n",
    "Substitute back in, we would have\n",
    "$$\\begin{align}\n",
    "F_b(x_i) &= F_{b-1}(x_i) + h(x_i) \\nonumber \\\\\n",
    "         &= F_{b-1}(x_i) + y_i - F_{b-1}(x_i) \\nonumber \\\\\n",
    "         &= F_{b-1}(x_i) - \\frac{ \\partial L\\big(y_i, F_{b-1}(x_i)\\big) }{ \\partial F_{b-1}(x_i) }\n",
    "         \\nonumber \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Hence, to obtain a $F_{b-1}(x_i)$ that is $F_{b-1}(x_i) = F_{b}(x_i) = y$, we are essentially minimizing $- \\eta \\times \\nabla L\\big(y_i, F(x_i)\\big)$. Rearrange the equation, we would have $F_b(x_i) - F_{b-1}(x_i) = - \\frac{ \\partial L\\big(y_i, F_{b-1}(x_i)\\big) }{ \\partial F_{b-1}(x_i) }$, which essentially is $y_i - F_{b-1}(x_i) = - \\frac{ \\partial L\\big(y_i, F_{b-1}(x_i)\\big) }{ \\partial F_{b-1}(x_i) }$. Now in code implementation, all we have to do is to minimize $y_i - F_{b-1}(x_i)$, which we can refer as **residual**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680d255-d001-4f9b-a6fa-66f253f4e732",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680752e-362c-4084-8dd2-f52506610fc2",
   "metadata": {},
   "source": [
    "When it is classification, we are considering $L$ to be cross entropy loss:\n",
    "$$L\\big(y_i, F(x_i)\\big) = -\\sum_k ^ K y_k(x_i) \\log p_k(x_i)$$\n",
    "$y_k(x_i)$ will be a one hot vector of length $k$, with $1$ as the $i$th entry where $y_i = k$ and $0$ everywhere else, and $p_k(x_i)$ would be the predicted probability of sample $x_i$ belonging to class $k$ by our model.\n",
    "\n",
    "To obtain $p$, we can perform softmax transformation on $o$ which is the raw output (logits) from our model. Referencing how we handle this in logistic regression, we can know that\n",
    "$$\n",
    "\\frac{\\partial p_i}{\\partial o_j} = \\frac{\\partial \\frac{e^{o_i}}{\\sum_{k=1}^{N}e^{o_k}}}{\\partial o_j}\n",
    "$$\n",
    "knowing that $f'(x) = \\frac{g'(x)h(x) - h'(x)g(x)}{[h(x)]^2}$ and $$\\begin{align*}\n",
    "g &= e^{o_i} \\nonumber \\\\\n",
    "h &= \\sum_{k=1}^{K}e^{o_k} \\nonumber\n",
    "\\end{align*}$$\n",
    "when $i = j$ we have\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\frac{e^{o_i}}{\\sum_{k = 1}^{N} e^{o_k}}}{\\partial o_j} \n",
    "&= \\frac{e^{o_i}\\Sigma-e^{o_j}e^{o_i}}{\\Sigma^2} \\nonumber \\\\\n",
    "&= \\frac{e^{o_i}}{\\Sigma}\\frac{\\Sigma - e^{o_j}}{\\Sigma} \\nonumber \\\\\n",
    "&= p_i(1 - p_j) \\nonumber \\\\\n",
    "&= p_i(1 - p_i) \\nonumber\n",
    "\\end{align*}$$,\n",
    "and when $ i \\neq j$, we have\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\frac{e^{o_i}}{\\sum_{k = 1}^{N} e^{o_k}}}{\\partial o_j} \n",
    "&= \\frac{0-e^{o_j}e^{o_i}}{\\Sigma^2} \\nonumber \\\\\n",
    "&= -\\frac{e^{o_j}}{\\Sigma}\\frac{e^{o_i}}{\\Sigma} \\nonumber \\\\\n",
    "&= -p_j p_i \\nonumber \\\\\n",
    "&= -p_i p_j \\nonumber\n",
    "\\end{align*}$$\n",
    "\n",
    "Plug them all back in, we have\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial o_i} \n",
    "&= -\\sum_k y_k\\frac{\\partial \\log p_k}{\\partial o_i} \\nonumber \\\\\n",
    "&= -\\sum_k y_k\\frac{1}{p_k}\\frac{\\partial p_k}{\\partial o_i} \\nonumber \\\\\n",
    "&= -y_i(1-p_i) - \\sum_{k \\neq i}y_k\\frac{1}{p_k}(-p_kp_i) \\nonumber \\\\\n",
    "&= -y_i(1 - p_i) + \\sum_{k \\neq i}y_k(p_i) \\nonumber \\\\\n",
    "&= -y_i + y_i p_i + \\sum_{k \\neq i}y_k(p_i) \\nonumber \\\\\n",
    "&= p_i\\left(\\sum_ky_k\\right) - y_i \\nonumber \\\\\n",
    "&= p_i - y_i \\nonumber\n",
    "\\end{align}$$\n",
    "\n",
    "The negative gradient is the **residual** in the case of classification, which is $y_i - F_{b-1}(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda5c28-68fe-440d-9dbb-83fa65804184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
