{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "68097880-cdb5-48ec-b198-217bd077d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb1001-5dd7-4f9b-bc91-818351e74b11",
   "metadata": {},
   "source": [
    "# Why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271b471-14e5-4563-80c9-a2d5e2681641",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "We want model to output classes rather than meaningless floating point values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e27cb-ee5a-4c6d-a846-95b697321f3e",
   "metadata": {},
   "source": [
    "<img src=\"why.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d2753-5b34-4dab-b121-ee325ee128e4",
   "metadata": {},
   "source": [
    "# How"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99c700-19cc-49b5-9817-c1223489c883",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9312f9-ee0f-4bfc-9733-c2a21e6ce7ea",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{odds($p$)} &= \\frac{p}{1-p}\\\\ \n",
    "\\text{log-odds($p$)} &= log(\\frac{p}{1-p})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a1233-31cf-4ed0-9020-3d0db725dea3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "t &= log(\\frac{p}{1-p})\\\\ \n",
    "e^{t} &= \\frac{p}{1-p}\\\\\n",
    "e^{t} - pe^{t} &= p\\\\\n",
    "p &= \\frac{e^{t}}{1+e^{t}} = \\frac{1}{1+e^{-t}}, t = x^{T}\\theta\\\\\n",
    "P(Y = 1 | x) &= \\frac{1}{1 + e^{-x^{T}\\theta}} = \\sigma (x^{T}\\theta)\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a24bf5-ee42-47d9-a909-e5720041e8f8",
   "metadata": {},
   "source": [
    "This just looks like a linear model, so we wrap $\\sigma$ around it and call LR a GLM, because it is a non-linear transformation of a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9fef34-2f5d-4282-8211-51162523ec26",
   "metadata": {},
   "source": [
    "### Properties of logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874997b3-9a7e-4138-9bf2-7a5b92cc6abf",
   "metadata": {},
   "source": [
    "Definition: $\\sigma(t) = \\frac{1}{1+e^{-t}} = \\frac{e^t}{1+e^t}$  \n",
    "Range: $0 < \\sigma(t) < 1$  \n",
    "Inverse: $t = \\sigma^{-1}(p) = \\log\\left(\\frac{p}{1-p}\\right)$  \n",
    "Reflection and Symmetry: $1 - \\sigma(t) = \\frac{e^{-t}}{1+e^{-t}} = \\sigma(-t)$  \n",
    "Derivative: $\\frac{d}{dt} \\sigma(t) = \\sigma(t)(1-\\sigma(t)) = \\sigma(t) \\sigma(-t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510506-f070-4ae1-b050-12dfe0d6a71b",
   "metadata": {},
   "source": [
    "### Parameter interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988c335-2c20-469f-84fe-700c56d28ec6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "t &= log(\\frac{p}{1-p})\\\\ \n",
    "e^{x^{T}\\theta} &= \\frac{P(Y = 1|x)}{P(Y = 0|x)}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ac002-b939-4cd1-978e-0a22fe72954a",
   "metadata": {},
   "source": [
    "### Loss function choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de52582-4a02-478a-8dfc-7509a53e7f13",
   "metadata": {},
   "source": [
    "**MSE is no good because**\n",
    "Loss is not convex\n",
    "<img src=\"mse.jpeg\">\n",
    "and loss is not large enough to penalize\n",
    "<img src=\"mse_penalize.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32135619-05a0-4805-af11-2a4f010c4946",
   "metadata": {},
   "source": [
    "**Cross-entropy** comes to rescue. Negative log loss can goes to infinity on wrong predictions.\n",
    "<img src=\"nll.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b381ebe-33d3-47b8-8bb6-04fd8308debd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{loss} = \n",
    "\\begin{cases} \n",
    "-\\log(1-\\hat{y}) & \\text{if } y = 0 \\\\\n",
    "-\\log(\\hat{y}) & \\text{if } y = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "\\text{loss} = -y log(\\hat{y}) - (1-y)log(1-\\hat{y})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2791abc-689b-4e5f-a85f-d5e6ca11a407",
   "metadata": {},
   "source": [
    "### Softmax and Cross-Entropy Loss Derivation\n",
    "\n",
    "#### **Softmax Function Definition:**\n",
    "\n",
    "The softmax function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "\n",
    "where $( z_i )$ is the logit corresponding to the $( i )$-th class and $( K )$ is the total number of classes.\n",
    "\n",
    "#### **Cross-Entropy Loss Definition:**\n",
    "\n",
    "The cross-entropy loss for a multi-class classification problem is defined as:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^K y_i \\log(\\hat{p}_i)\n",
    "$$\n",
    "\n",
    "where $( y_i )$ is the true label in one-hot encoded format $( y_i = 1 )$ for the correct class and 0 otherwise, and $( \\hat{p}_i )$ is the predicted probability for class $( i )$, given by the softmax of $( z_i )$.\n",
    "\n",
    "#### **Gradient Derivation:**\n",
    "\n",
    "We need to find how the loss $( L )$ changes with each logit $( z_j )$. This involves using the chain rule for partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial \\hat{p}_i} \\frac{\\partial \\hat{p}_i}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "The partial derivatives are calculated as follows:\n",
    "\n",
    "- **Loss with respect to the predicted probabilities:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{p}_i} = -\\frac{y_i}{\\hat{p}_i}\n",
    "$$\n",
    "\n",
    "- **Predicted probabilities with respect to logits:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial z_j}log(\\hat{p}_i) &= \\frac{1}{\\hat{p}_i} \\cdot \\frac{\\partial \\hat{p}_i}{\\partial z_j}\\\\\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} &= \\hat{p}_i \\cdot \\frac{\\partial}{\\partial z_j}log(\\hat{p}_i) \\\\\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} &= \\hat{p}_i \\cdot \\frac{\\partial}{\\partial z_j}log(\\frac{e^{z_i}}{\\sum_{k=1}^K e^{z_k}}) \\\\\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} &= \\hat{p}_i \\cdot \\frac{\\partial}{\\partial z_j}(log(e^{z_i}) - log(\\sum_{k=1}^K e^{z_k})) \\\\\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} &= \\hat{p}_i \\cdot \\frac{\\partial}{\\partial z_j}(z_i - log(\\sum_{k=1}^K e^{z_k})) \\\\\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} &= \\hat{p}_i \\cdot (\\frac{\\partial z_i}{\\partial z_j} - \\frac{\\partial}{\\partial z_j}log(\\sum_{k=1}^K e^{z_k})) \\\\\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} &= \\hat{p}_i \\cdot (\\frac{\\partial z_i}{\\partial z_j} - (\\frac{1}{\\sum_{k=1}^K e^{z_k}})\\frac{\\partial}{\\partial z_j}(\\sum_{k=1}^K e^{z_k})) \\\\\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} &= \\hat{p}_i \\cdot (\\frac{\\partial z_i}{\\partial z_j} - (\\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}})) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For $( i = j )$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} = \\hat{p}_i (1 - \\hat{p}_j)\n",
    "$$\n",
    "\n",
    "For $( i \\neq j )$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{p}_i}{\\partial z_j} = -\\hat{p}_i \\hat{p}_j\n",
    "$$\n",
    "\n",
    "#### **Combining These Results:**\n",
    "\n",
    "Combine the partial derivatives into the final gradient formula:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial z_j} &= \\sum_{i=1}^K -\\frac{y_i}{\\hat{p}_i} \\frac{\\partial \\hat{p}_i}{\\partial z_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = -\\frac{y_j}{\\hat{p}_j} \\hat{p}_j (1 - \\hat{p}_j) + \\sum_{i \\neq j} -\\frac{y_i}{\\hat{p}_i} (-\\hat{p}_i \\hat{p}_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = -y_j (1 - \\hat{p}_j) + \\sum_{i \\neq j} y_i \\hat{p}_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = -y_j + y_j \\hat{p}_j + \\hat{p}_j \\sum_{i \\neq j} y_i = \\hat{p}_j - y_j\n",
    "$$\n",
    "\n",
    "because $( \\sum_{i=1}^K y_i = 1 )$ (since $( y_i )$ is one-hot encoded).\n",
    "\n",
    "#### **Final Gradient Result:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\hat{p}_j - y_j\n",
    "$$\n",
    "\n",
    "This gradient indicates the direction and magnitude by which the logits $( z_j )$ need to be adjusted to minimize the loss, facilitating the update of weights during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90589ca1-4871-4034-a02f-9fe3b207e914",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "88281006-a3ba-4d23-b93e-a1f6db36f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "8df8908f-dbc2-4fcb-aeb5-4fa75dfc8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    def __init__(\n",
    "            self, max_iter=100, learning_rate=0.01, \n",
    "            decision_thres=0.5, regularization='l2', \n",
    "            lambda_reg=0.01, batch_size=32, momentum=0.9,\n",
    "            random_state=None\n",
    "        ):\n",
    "        self.decision_thres = decision_thres\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.batch_size = batch_size\n",
    "        self.momentum = momentum\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        # Stable softmax function\n",
    "        exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-16, 1 - 1e-16)\n",
    "        base_loss = (-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "        if self.regularization == 'l2':\n",
    "            reg_loss = 0.5 * self.lambda_reg * np.sum(self.weights ** 2)\n",
    "        elif self.regularization == 'l1':\n",
    "            reg_loss = self.lambda_reg * np.sum(np.abs(self.weights))\n",
    "        else:\n",
    "            reg_loss = 0\n",
    "        return base_loss + reg_loss\n",
    "\n",
    "    def _derivative_ce_loss(self, y, z):\n",
    "        sigma_z = self._softmax(z)\n",
    "        grad = sigma_z - y  # The simplified derivative of loss with respect to z\n",
    "        return grad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = np.max(y) + 1\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "        self.weights = np.random.normal(scale=0.1, size=(n_features, n_classes))\n",
    "        self.bias = np.random.normal(scale=0.1, size=n_classes)\n",
    "\n",
    "        v_weights = np.zeros((n_features, n_classes))\n",
    "        v_bias = np.zeros(n_classes)\n",
    "        \n",
    "        indices = list(range(n_samples))\n",
    "        np.random.shuffle(indices)\n",
    "        indices = cycle(indices)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            if self.batch_size:\n",
    "                batch = [next(indices) for _ in range(self.batch_size)]\n",
    "                X_batch, y_batch = X[batch], y[batch]\n",
    "            else:\n",
    "                X_batch, y_batch = X, y\n",
    "            \n",
    "            logits = X_batch @ self.weights + self.bias\n",
    "            dloss = self._derivative_ce_loss(np.eye(n_classes)[y_batch], logits)\n",
    "            dw = X_batch.T @ dloss / n_samples\n",
    "            db = dloss.mean()\n",
    "    \n",
    "            # Regularization updates\n",
    "            if self.regularization == 'l2':\n",
    "                dw += self.lambda_reg * self.weights\n",
    "            elif self.regularization == 'l1':\n",
    "                dw += self.lambda_reg * np.sign(self.weights)\n",
    "\n",
    "            v_weights = self.momentum * v_weights + self.learning_rate * dw\n",
    "            v_bias = self.momentum * v_bias + self.learning_rate * db\n",
    "\n",
    "            self.weights -= v_weights\n",
    "            self.bias -= v_bias\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        logits = X @ self.weights + self.bias\n",
    "        return self._softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_prob(X), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return (self.predict(X) == y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "4ea7da9b-b93e-4964-b0b4-e18ccf3551d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (first 5 samples):\n",
      " [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "   0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n",
      " [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323  0.33367433\n",
      "   1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462 -1.45436567\n",
      "   0.04575852 -0.18718385  1.53277921  1.46935877]\n",
      " [ 0.15494743  0.37816252 -0.88778575 -1.98079647 -0.34791215  0.15634897\n",
      "   1.23029068  1.20237985 -0.38732682 -0.30230275]\n",
      " [-1.04855297 -1.42001794 -1.70627019  1.9507754  -0.50965218 -0.4380743\n",
      "  -1.25279536  0.77749036 -1.61389785 -0.21274028]]\n",
      "Labels (first 5 samples): [48 37 45 25 32]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    # Stable softmax function\n",
    "    exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "# Generate random data\n",
    "num_samples = 1000000\n",
    "num_features = 10\n",
    "num_classes = 50\n",
    "\n",
    "# Random features\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "\n",
    "# Generate labels based on a linear combination of features with noise\n",
    "weights = np.random.randn(num_features, num_classes)  # Arbitrary weights for features\n",
    "bias = np.random.randn(num_classes)  # Some bias\n",
    "noise = np.random.normal(0, 0.05, (num_samples, num_classes))  # Noise to make the task not perfectly linear\n",
    "\n",
    "# Linear combination to generate continuous values\n",
    "linear_combination = X.dot(weights) + bias + noise\n",
    "\n",
    "probabilities = softmax(linear_combination)\n",
    "\n",
    "# Binarize probabilities to get binary labels\n",
    "# threshold = 0.5\n",
    "# labels = (probabilities > threshold).astype(int)\n",
    "labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Display the first few samples\n",
    "print(\"Features (first 5 samples):\\n\", X[:5])\n",
    "print(\"Labels (first 5 samples):\", labels[:5])\n",
    "\n",
    "indices = list(range(num_samples))\n",
    "np.random.shuffle(indices)\n",
    "X_train, X_test = X[indices[:int(num_samples*0.8)]], X[indices[int(num_samples*0.8):]]\n",
    "labels_train, labels_test = labels[indices[:int(num_samples*0.8)]], labels[indices[int(num_samples*0.8):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "id": "2759dcbc-6ea5-424d-bf8b-4614e1c2dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR(max_iter=100, learning_rate=2, lambda_reg=0.0001, batch_size=20480, momentum=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "3fca555c-d25e-448c-b520-128d29171ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.189581871032715 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "lr.fit(X_train, labels_train)\n",
    "time_end = time.time()\n",
    "\n",
    "print(f\"{time_end-time_start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "3edd0bc6-fb15-4074-b1d5-b2f5a843fa60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.679065"
      ]
     },
     "execution_count": 983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "637f8b36-4bc1-47f0-ae5c-e17b780b7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "id": "ee83f95f-65d6-4494-916a-3c8873749711",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_lr = SGDClassifier(max_iter=50, random_state=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "c6c80b07-a81e-4420-8788-cfef2c5c34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.057495832443237 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "sklearn_lr.fit(X_train, labels_train)\n",
    "time_end = time.time()\n",
    "\n",
    "print(f\"{time_end-time_start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "49a4d6c8-5eb2-4023-b4c6-83072e4e2f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66023"
      ]
     },
     "execution_count": 956,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lr.score(X_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12155fe4-5862-4736-8193-4117b4eb94b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
